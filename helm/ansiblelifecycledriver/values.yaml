## Docker Image for the osslm-ansible-rm application
docker:
  ## Make this the full path, including registry host and port if using one
  image: accanto/ansible-lifecycle-driver
  version: 2.1.0.dev0
  imagePullPolicy: IfNotPresent

## Configuration for the application deployment
app:
  ## Number of pods to deploy 
  replicas: 1
  
  ## Probe configuration for checking Application availability
  livenessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 30
  readinessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 20
    periodSeconds: 10
    
  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity:
    podAntiAffinity:
      ## Default anti-affinity rule is to try and schedule multiple pods of this app on different nodes
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: "kubernetes.io/hostname"
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - ansible-lifecycle-driver
    ## Example of node affinity rule to require this pod is only scheduled on a Node with a label (key=value) of "Stateless=True"
    #nodeAffinity:
    #  requiredDuringSchedulingIgnoredDuringExecution:
    #    nodeSelectorTerms:
    #    - matchExpressions:
    #      - key: Stateless
    #        operator: In
    #        values:
    #        - "True"
    
  ## Node tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  ## Example of allowing this pod to be deployed onto a Node that has been tainted with "ReservedForLm"
  #- key: ReservedForLm
  #  operator: Exists
  #  effect: NoSchedule

  ## Configure resource requests and limits for each container of the application
  ## Default is to have no limit
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #  cpu: 250m
    #  memory: 384Mi
    # requests:
    #  cpu: 250m
    #  memory: 384Mi
  
  ## Autoscaler
  ## Enabling this deploys a Horizontal Pod Autoscaler that will monitor the CPU usage and scale this deployment 
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## NOTE: requires Kubernetes Metrics Server
  autoscaler:
    enabled: false
    maxReplicas: 10
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
    
  ## ansible-lifecycle-driver specific configuration
  config:
    kafka:
      ## note: assumes that Kafka is accessible at 'app.config.override.messaging.connection_address'

      ## There is an additional option for configuring access to Kafka by fixed IP. This is added as a host entry to the container.
      hostEnabled: false
      ## the host must match the value of 'app.config.override.messaging.connection_address', if it is set
      host: "foundation-kafka"
      ## this must be set to the Kafka IP address
      ip:

    ## Configure logging 
    log:
      level: INFO

    ## Pass additional environment variables to the application containers
    env:
      ## to support indexing of logs in Elasticsearch using Filebeat, we use logstash format. This allows
      ## us to bundle the log message and other metadata in a json log message and let Fielbeat extract them
      ## as top level fields in the Elasticsearch index.
      LOG_TYPE: logstash

      ## configuration for WSGI container
      ## WSGI_CONTAINER can be uwsgi or gunicorn
      WSGI_CONTAINER: gunicorn
      ## the number of processes and threads to spawn to handle requests
      ## number of processes is 1 because the driver handles process management
      NUM_PROCESSES: "1"
      # Required only when WSGI_CONTAINER is uwsgi
      #NUM_THREADS: "2"

    ingress:
      ## The host used to access the service externally i.e. http://<server.ingress.hostname>/
      ## If not making use of a DNS to resolve this host you must add this to the client hosts file with the K8s IP
      ## E.g.
      ## 10.220.1.2     ald.lm
      host: ald.lm

    ## ald-config.yml (driver configuration) overrides
    override:
      ansible:
        ## unreachability retries
        ## sleep between retries (in seconds)
        unreachable_sleep_seconds: 5
        ## maximum number of retries before a failure
        max_unreachable_retries: 60

        ## output properties are set using set_fact in the Ansible script and prefixed with this string
        ## so that the driver knows they are intended to be exported to Brent.
        output_prop_prefix: 'output__'

      process:
        ## whether to a process pool to read and process transition requests
        use_process_pool: True
        ## settings for use_process_pool == True (a pool of processes handles lifecycle requests)
        ### size of the Ansible process pool
        process_pool_size: 10

      messaging:
        connection_address: foundation-kafka:9092

      resource_driver:
        ## workspace for holding lifecycle scripts
        scripts_workspace: /var/ald/lifecycle_scripts
        ## enable to prevent scripts from being removed after use
        ## care should be taken when enabling this option, as the pod will quickly run out of disk space
        ## enable only for debugging purposes
        keep_files: False
        async_messaging_enabled: False
        lifecycle_request_queue:
          enabled: True
          group_id: ald_request_queue
          ## Set this to higher than the maximum time you'd expect a request to be executed (1 hour)
          max_poll_interval_ms: 3600000
          topic:
            auto_create: True
            num_partitions: 100
            replication_factor: 1
            config:
              # 1 hour
              retention.ms: 3600000
              message.timestamp.difference.max.ms: 3600000
              file.delete.delay.ms: 3600000
          failed_topic:
            auto_create: True
            num_partitions: 1
            replication_factor: 1
    security:
      ssl:
        enabled: True
        secret:
          ## Name of the secret containing the SSL certificate for the non-host based access (the CN of the certificate should match commonName)
          name: ald-tls
          ## If True, the Helm chart installation will generate a new SSL key with a self-signed certificate
          generate: True
          ## The Common Name used for the SSL certificate (do not change this)
          commonName: ansible-lifecycle-driver

service:
  # Using NodePort allows access to the IPs through http://k8s-host:nodePort/
  type: NodePort
  nodePort: 31680
